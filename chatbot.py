import os
import gradio as gr
from dotenv import load_dotenv
import threading
import time
import logging

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from langchain_helper import create_chat_handler, get_available_models
    from document_processor import get_document_processor
    from file_watcher import get_file_watcher
    from rag_chain import get_rag_chain
    
    has_langchain = True
    has_document_processor = True
    has_file_watcher = True
    has_rag_chain = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥æŸäº›æ¨¡å—: {e}")
    has_langchain = 'langchain_helper' not in str(e)
    has_document_processor = 'document_processor' not in str(e)
    has_file_watcher = 'file_watcher' not in str(e)
    has_rag_chain = 'rag_chain' not in str(e)

# å¯¼å…¥ç¯å¢ƒå˜é‡ç®¡ç†å™¨
try:
    from env_manager import get_env_manager
    env_manager = get_env_manager()
except ImportError:
    env_manager = None
    print("è­¦å‘Š: ç¯å¢ƒå˜é‡ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å…¨å±€å˜é‡
gradio_app = None
gradio_thread = None
chat_handler = None
document_processor = None
file_watcher = None
rag_chain = None

def setup_components():
    """åˆå§‹åŒ–ç»„ä»¶"""
    global chat_handler, document_processor, file_watcher, rag_chain
    
    logger.info("å¼€å§‹åˆå§‹åŒ–ç»„ä»¶...")
    
    # åˆå§‹åŒ–èŠå¤©å¤„ç†å™¨
    if has_langchain:
        try:
            model_type = os.getenv("LLM_MODEL_TYPE", "openai")
            logger.info(f"æ­£åœ¨åˆ›å»ºèŠå¤©å¤„ç†å™¨ï¼Œæ¨¡å‹ç±»å‹: {model_type}, API Key: {os.getenv('OPENAI_API_KEY')[:5]}***")
            chat_handler = create_chat_handler(model_type)
            logger.info(f"æˆåŠŸåˆ›å»ºèŠå¤©å¤„ç†å™¨ï¼Œä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")
        except Exception as e:
            logger.error(f"åˆ›å»ºèŠå¤©å¤„ç†å™¨å¤±è´¥: {e}")
            chat_handler = None
    
    # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
    if has_document_processor:
        try:
            logger.info("æ­£åœ¨åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨...")
            document_processor = get_document_processor()
            logger.info("æˆåŠŸåˆ›å»ºæ–‡æ¡£å¤„ç†å™¨")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨å¤±è´¥: {e}")
            document_processor = None
    
    # åˆå§‹åŒ–æ–‡ä»¶ç›‘å¬å™¨
    if has_file_watcher and document_processor:
        try:
            logger.info("æ­£åœ¨åˆ›å»ºæ–‡ä»¶ç›‘å¬å™¨...")
            file_watcher = get_file_watcher()
            logger.info("æˆåŠŸåˆ›å»ºæ–‡ä»¶ç›‘å¬å™¨")
            
            # å¤„ç†å·²å­˜åœ¨çš„æ–‡æ¡£
            logger.info("å¤„ç†å·²å­˜åœ¨çš„æ–‡æ¡£...")
            file_watcher.process_existing_documents()
            
            # å¯åŠ¨æ–‡ä»¶ç›‘å¬
            logger.info("å¯åŠ¨æ–‡ä»¶ç›‘å¬...")
            if file_watcher.start():
                logger.info("æ–‡ä»¶ç›‘å¬å™¨å·²å¯åŠ¨")
        except Exception as e:
            logger.error(f"åˆ›å»ºæˆ–å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨å¤±è´¥: {e}")
            file_watcher = None
    
    # åˆå§‹åŒ–RAGé“¾
    if has_rag_chain and document_processor:
        try:
            logger.info("æ­£åœ¨åˆ›å»ºRAGé“¾...")
            rag_chain = get_rag_chain()
            logger.info("æˆåŠŸåˆ›å»ºRAGé“¾")
        except Exception as e:
            logger.error(f"åˆ›å»ºRAGé“¾å¤±è´¥: {e}")
            rag_chain = None
    
    # è¿”å›ç»„ä»¶çŠ¶æ€
    return {
        "chat_handler": chat_handler is not None,
        "document_processor": document_processor is not None,
        "file_watcher": file_watcher is not None,
        "rag_chain": rag_chain is not None
    }

def create_chatbot_ui():
    """
    åˆ›å»ºGradioèŠå¤©ç•Œé¢
    """
    # ç¡®ä¿gradioå¯ç”¨
    try:
        import gradio as gr
    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥gradioæ¨¡å—ï¼Œè¯·ç¡®ä¿å·²å®‰è£…")
        return None
        
    # åˆå§‹åŒ–ç»„ä»¶
    components_status = setup_components()
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_type = os.getenv("LLM_MODEL_TYPE", "openai")
    model_name = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo") if model_type == "openai" else os.getenv("HUGGINGFACE_MODEL", "æ¨¡å‹æœªè®¾ç½®")
    
    try:
        with gr.Blocks(
            title="ç”»å»Šæ™ºèƒ½åŠ©æ‰‹",
            css="""
            .gradio-container {background-color: transparent !important}
            .chat-message {margin-bottom: 10px; padding: 8px 12px; border-radius: 15px;}
            .user-message {background-color: #e1f5fe; align-self: flex-end;}
            .bot-message {background-color: #f5f5f5; align-self: flex-start;}
            #component-0 {min-height: 400px; height: 100%;}
            .footer {font-size: 0.85em; color: #666; text-align: center; margin-top: 5px;}
            .status-badge {display: inline-block; padding: 2px 6px; border-radius: 3px; font-size: 0.8em; margin-right: 5px;}
            .status-success {background-color: #e8f5e9; color: #2e7d32;}
            .status-error {background-color: #ffebee; color: #c62828;}
            .source-documents {font-size: 0.85em; color: #555; margin-top: 5px; padding: 5px; background-color: #f5f5f5; border-radius: 4px;}
            """
        ) as demo:
            # çŠ¶æ€å˜é‡
            chatbot = gr.State([])
            
            with gr.Column():
                # ç»„ä»¶çŠ¶æ€æ˜¾ç¤º
                status_html = ""
                if components_status["chat_handler"]:
                    status_html += "<span class='status-badge status-success'>LLM âœ“</span>"
                else:
                    status_html += "<span class='status-badge status-error'>LLM âœ—</span>"
                
                if components_status["document_processor"]:
                    status_html += "<span class='status-badge status-success'>æ–‡æ¡£å¤„ç† âœ“</span>"
                else:
                    status_html += "<span class='status-badge status-error'>æ–‡æ¡£å¤„ç† âœ—</span>"
                
                if components_status["rag_chain"]:
                    status_html += "<span class='status-badge status-success'>RAG âœ“</span>"
                else:
                    status_html += "<span class='status-badge status-error'>RAG âœ—</span>"
                
                # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯å’Œç»„ä»¶çŠ¶æ€
                gr.Markdown(f"<div class='footer'>{status_html}<br>å½“å‰æ¨¡å‹: {model_name}</div>")
                
                # èŠå¤©å†å²æ˜¾ç¤ºåŒºåŸŸ
                chat_history = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=320,
                    show_copy_button=True,
                    avatar_images=(None, "ğŸ¤–"),
                )
                
                # è¾“å…¥åŒºåŸŸ
                with gr.Row():
                    user_input = gr.Textbox(
                        placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        container=False,
                        scale=9,
                    )
                    submit_btn = gr.Button("å‘é€", scale=1)
                    clear_btn = gr.Button("æ¸…ç©º", scale=1)
            
            # å®šä¹‰å›è°ƒå‡½æ•°
            def user_message(message, history):
                """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
                return "", history + [[message, None]]
            
            def bot_response(history):
                """ç”Ÿæˆæœºå™¨äººå“åº”"""
                if not history:
                    return history
                    
                message = history[-1][0]
                
                # ä½¿ç”¨RAGé“¾å¤„ç†æŸ¥è¯¢
                if has_rag_chain and rag_chain:
                    try:
                        # ä½¿ç”¨RAGé“¾æ‰§è¡ŒæŸ¥è¯¢
                        result = rag_chain.query(message)
                        response = result["answer"]
                        source_docs = result.get("source_documents", [])
                        
                        # å¦‚æœæœ‰å¼•ç”¨æºï¼Œæ·»åŠ åˆ°å›ç­”ä¸­
                        if source_docs:
                            source_info = "\n\n**å‚è€ƒæ¥æº:**\n"
                            for i, doc in enumerate(source_docs[:2]):
                                filename = doc.metadata.get('filename', 'æœªçŸ¥')
                                source_info += f"- {filename}\n"
                            
                            response += source_info
                        
                        history[-1][1] = response
                        
                    except Exception as e:
                        logger.error(f"RAGå¤„ç†å¤±è´¥: {e}")
                        history[-1][1] = f"æŠ±æ­‰ï¼Œæ— æ³•ä½¿ç”¨çŸ¥è¯†åº“å›ç­”æ‚¨çš„é—®é¢˜: {str(e)}"
                
                # ä½¿ç”¨åŸºç¡€LLMå¤„ç†
                elif has_langchain and chat_handler:
                    try:
                        response = chat_handler.chat(message)
                        history[-1][1] = response
                    except Exception as e:
                        logger.error(f"LLMå¤„ç†å¤±è´¥: {e}")
                        history[-1][1] = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
                
                # å›é€€æ–¹æ¡ˆï¼šæ¨¡æ‹Ÿå“åº”
                else:
                    # æ¨¡æ‹Ÿå“åº”
                    time.sleep(1)
                    history[-1][1] = f"æ‚¨çš„æ¶ˆæ¯å·²æ”¶åˆ°ï¼š{message}ã€‚è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”ï¼Œè¯·é…ç½®LLMæˆ–RAGç³»ç»Ÿä»¥è·å–çœŸå®å›ç­”ã€‚"
                
                return history
            
            def clear_history():
                """æ¸…ç©ºèŠå¤©å†å²"""
                if has_rag_chain and rag_chain:
                    rag_chain.clear_history()
                elif has_langchain and chat_handler:
                    chat_handler.clear_history()
                return [], []
            
            # è®¾ç½®äº‹ä»¶è§¦å‘
            submit_btn.click(
                user_message,
                inputs=[user_input, chat_history],
                outputs=[user_input, chat_history],
                queue=False
            ).then(
                bot_response,
                inputs=[chat_history],
                outputs=[chat_history]
            )
            
            user_input.submit(
                user_message,
                inputs=[user_input, chat_history],
                outputs=[user_input, chat_history],
                queue=False
            ).then(
                bot_response,
                inputs=[chat_history],
                outputs=[chat_history]
            )
            
            clear_btn.click(
                clear_history,
                outputs=[chat_history, chatbot]
            )
        
        return demo
    except Exception as e:
        logger.error(f"åˆ›å»ºGradioç•Œé¢å¤±è´¥: {e}")
        return None

def start_gradio_server():
    """
    å¯åŠ¨GradioæœåŠ¡å™¨
    """
    global gradio_app
    
    # åˆ›å»ºèŠå¤©æœºå™¨äººç•Œé¢
    gradio_app = create_chatbot_ui()
    
    # å¦‚æœåˆ›å»ºç•Œé¢å¤±è´¥ï¼Œåˆ™è¿”å›
    if gradio_app is None:
        logger.error("æ— æ³•åˆ›å»ºGradioç•Œé¢ï¼ŒæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
        return False
    
    # è·å–Gradioç«¯å£
    gradio_port = int(os.getenv("GRADIO_PORT", 7860))
    
    # å¯åŠ¨æœåŠ¡å™¨
    try:
        logger.info(f"æ­£åœ¨å¯åŠ¨GradioæœåŠ¡å™¨ï¼Œç«¯å£: {gradio_port}")
        # ä½¿ç”¨ä½å¹¶å‘é…ç½®ï¼Œé¿å…Gradio 4.xçš„é˜Ÿåˆ—é—®é¢˜
        try:
            # ä»…åœ¨ç•Œé¢æ”¯æŒé˜Ÿåˆ—æ—¶ä½¿ç”¨é˜Ÿåˆ—
            gradio_app.queue(concurrency_count=1, max_size=10)
        except Exception as e:
            logger.warning(f"è®¾ç½®Gradioé˜Ÿåˆ—æ—¶å‡ºé”™: {e}")
            
        gradio_app.launch(
            server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
            server_port=gradio_port,
            share=False,
            inbrowser=False,
            debug=False
        )
        logger.info(f"GradioæœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç«¯å£: {gradio_port}")
        return True
    except Exception as e:
        logger.error(f"å¯åŠ¨GradioæœåŠ¡å™¨å¤±è´¥: {e}")
        return False

def get_gradio_app():
    """è·å–æˆ–åˆ›å»ºGradioåº”ç”¨å®ä¾‹"""
    global gradio_app
    if gradio_app is None:
        gradio_app = create_chatbot_ui()
    return gradio_app

def start_background_gradio():
    """åœ¨åå°å¯åŠ¨Gradio"""
    global gradio_thread
    
    try:
        logger.info("å¼€å§‹å¯åŠ¨åå°GradioæœåŠ¡...")
        
        # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦å·²å­˜åœ¨ä¸”æ´»è·ƒ
        if gradio_thread is not None and gradio_thread.is_alive():
            logger.info("Gradioçº¿ç¨‹å·²ç»åœ¨è¿è¡Œä¸­")
            return True

        # æ£€æŸ¥API Key
        api_key = os.getenv("OPENAI_API_KEY", "")
        if not api_key:
            logger.warning("æœªè®¾ç½®OpenAI API Keyï¼ŒGradioå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        else:
            api_preview = api_key[:5] + "****" if len(api_key) > 5 else "æœ‰æ•ˆä½†é•¿åº¦ä¸è¶³"
            logger.info(f"å·²æ£€æµ‹åˆ°API Key: {api_preview}")
            
        # åˆ›å»ºæ–°çš„çº¿ç¨‹
        logger.info("åˆ›å»ºGradioçº¿ç¨‹...")
        gradio_thread = threading.Thread(target=start_gradio_server)
        gradio_thread.daemon = True
        
        # å¯åŠ¨çº¿ç¨‹
        logger.info("å¯åŠ¨Gradioçº¿ç¨‹...")
        gradio_thread.start()
        
        # ç­‰å¾…Gradioå¯åŠ¨
        time.sleep(2)
        
        if gradio_thread.is_alive():
            logger.info("Gradioå·²åœ¨åå°çº¿ç¨‹å¯åŠ¨")
            return True
        else:
            logger.error("Gradioçº¿ç¨‹å¯åŠ¨å¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"å¯åŠ¨åå°GradioæœåŠ¡å¤±è´¥: {str(e)}")
        return False

def cleanup():
    """æ¸…ç†èµ„æº"""
    global file_watcher
    
    # åœæ­¢æ–‡ä»¶ç›‘å¬å™¨
    if file_watcher:
        try:
            file_watcher.stop()
            logger.info("æ–‡ä»¶ç›‘å¬å™¨å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢æ–‡ä»¶ç›‘å¬å™¨å¤±è´¥: {e}")

if __name__ == "__main__":
    try:
        # å¯åŠ¨Gradio
        start_gradio_server()
        
        # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
        cleanup()
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶å‡ºé”™: {e}")
        cleanup() 